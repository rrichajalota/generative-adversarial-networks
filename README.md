# generative-adversarial-networks

For training the images, a GPU enabled cloud platform named ‘floydhub’ was used. The code for DCGAN was taken from [Siraj Raval's Repository](https://github.com/llSourcell/how_to_generate_video) for generating videos. 

## Case 1:

* Input Images: 700
* No. of Epochs: 1830
* Batch size: 100
* No. of convolutional layers in Generator and Discriminator: 2
* Optimizer used in both G and D: SGD
* Training time: 18 hours 
* Equilibrium point: 1630th epoch

![clean image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%201/clean.png)  ![Dirty Image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%201/dirty.png)

### Analysis:

We see that the images generated by the generator after 1830 epochs resemble a human face. The features like hair, eyes, nose, chin can be easily spotted in the images above. However, there is still too much noise in the output and there is a lot of scope for improvement. Looking at the training images, we can conclude that, the generator starts with random noise and then learns slowly from its mistakes. We find that after a certain number of epochs, the training images start showing up poorly with destroyed facial features. This happens because the training continues beyond the equilibrium point. In this case, the equilibrium point can be considered to be around 1630th epoch. 

## Case 2:

* Input Images: 700
* No. of Epochs: 2500
* Batch size: 100
* No. of convolutional layers in Generator and Discriminator: 3
* Optimizer used in both G and D: SGD
* Training time: 22 hours 
* Equilibrium point: 1250th epoch

![clean image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%202/clean.png) ![dirty image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%202/dirty.png)

### Analysis:

We see that the images generated by the generator after 2500 epochs very faintly resemble a human face. Only the features like hair and an undefined oval face can be seen in the images above. These images are nosier than the images generated by our previous model. One of the possible reasons for these distorted outputs could be that the training was stopped way after the optimum point was reached. The generator already started disrupting the good weights when the training was stopped. Other possible reason could be that our added modification simply adds noise to the network and the generator starts picking it up as a feature. We can verify this by looking at the training samples generated by the model. Even in these samples there is too much noise, therefore noise in the generated output is completely justified. For this model, the equilibrium point can be considered to be around 1250th epoch, i.e. if we had stopped training after that epoch, we would have generated better images.  

## Case 3:

* Input Images: 700
* No. of Epochs: 1700
* Batch size: 100
* No. of convolutional layers in Generator and Discriminator: 2
* Optimizer used in G and D: Adam and SGD
* Training time: 14 hours 
* Optimum point: 1540th epoch

![clean image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%203/clean.png) ![dirty image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%203/dirty.png)

### Analysis

We see that the images generated by the generator after 1700 epochs resemble a human face. The features like hair, eyes (only one though), nose, chin and even lips can be identified in the images above. But still there is a lot of noise in the output images and the model needs to learn a corect human face representation. Had the generator been trained for some more epochs or stopped at the previously attained equilibrium point, the model would have generated a better image than the previous cases. This is because in the output images above an important facial feature (lips), which was missed by the models in the previous cases, also gets generated. We find that after a certain number of epochs, the training images start showing up poorly with destroyed facial features. This happens because the training continues beyond the equilibrium point. In this case, the optimal point can be considered to be around 1540th epoch. Had we stopped our training then, we would have hopefully got clearer, less noisier and correct face images.
