# generative-adversarial-networks

For training the images, a GPU enabled cloud platform named ‘floydhub’ was used. The code for DCGAN was taken from [Siraj Raval's Repository](https://github.com/llSourcell/how_to_generate_video) for generating videos. 

![training gif](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%203/training)

## Case 1:

* Input Images: 700
* No. of Epochs: 1830
* Batch size: 100
* No. of convolutional layers in Generator and Discriminator: 2
* Optimizer used in both G and D: SGD
* Training time: 18 hours 
* Equilibrium point: 1630th epoch

![clean image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%201/clean.png)  ![Dirty Image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%201/dirty.png)

### Analysis:

We see that the images generated by the generator after 1830 epochs resemble a human face. The features like hair, eyes, nose, chin can be easily spotted in the images above. However, there is still too much noise in the output and there is a lot of scope for improvement. Looking at the training images, we can conclude that, the generator starts with random noise and then learns slowly from its mistakes. We find that after a certain number of epochs, the training images start showing up poorly with destroyed facial features. This happens because the training continues beyond the equilibrium point. In this case, the equilibrium point can be considered to be around 1630th epoch. 

## Case 2:

* Input Images: 700
* No. of Epochs: 2500
* Batch size: 100
* No. of convolutional layers in Generator and Discriminator: 3
* Optimizer used in both G and D: SGD
* Training time: 22 hours 
* Equilibrium point: 1250th epoch

![clean image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%202/clean.png) ![dirty image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%202/dirty.png)

### Analysis:

We see that the images generated by the generator after 2500 epochs very faintly resemble a human face. Only the features like hair and an undefined oval face can be seen in the images above. These images are nosier than the images generated by our previous model. One of the possible reasons for these distorted outputs could be that the training was stopped way after the optimum point was reached. The generator already started disrupting the good weights when the training was stopped. Other possible reason could be that our added modification simply adds noise to the network and the generator starts picking it up as a feature. We can verify this by looking at the training samples generated by the model. Even in these samples there is too much noise, therefore noise in the generated output is completely justified. For this model, the equilibrium point can be considered to be around 1250th epoch, i.e. if we had stopped training after that epoch, we would have generated better images.  

## Case 3:

* Input Images: 700
* No. of Epochs: 1700
* Batch size: 100
* No. of convolutional layers in Generator and Discriminator: 2
* Optimizer used in G and D: Adam and SGD
* Training time: 14 hours 
* Optimum point: 1540th epoch

![clean image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%203/clean.png) ![dirty image](https://github.com/rrichajalota/generative-adversarial-networks/blob/master/case%203/dirty.png)

### Analysis

We see that the images generated by the generator after 1700 epochs resemble a human face. The features like hair, eyes (only one though), undefined nose, chin and lips can be identified in the images above. The results obtained in the case 1 prove to be much better than the ones obtained after applying modifications. We find that after a certain number of epochs, the training images start showing up poorly with destroyed facial features. This happens because the training continues beyond the equilibrium point. In this case, the optimal point can be considered to be around 1540th epoch. Had we stopped our training then, we would have hopefully got clearer and less noisier images.
